{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key=os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-3.5-turbo-16k-0613</td>\n",
       "      <td>model</td>\n",
       "      <td>1685474247</td>\n",
       "      <td>openai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dall-e-3</td>\n",
       "      <td>model</td>\n",
       "      <td>1698785189</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text-embedding-3-large</td>\n",
       "      <td>model</td>\n",
       "      <td>1705953180</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dall-e-2</td>\n",
       "      <td>model</td>\n",
       "      <td>1698798177</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whisper-1</td>\n",
       "      <td>model</td>\n",
       "      <td>1677532384</td>\n",
       "      <td>openai-internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tts-1-hd-1106</td>\n",
       "      <td>model</td>\n",
       "      <td>1699053533</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tts-1-hd</td>\n",
       "      <td>model</td>\n",
       "      <td>1699046015</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>model</td>\n",
       "      <td>1706048358</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>model</td>\n",
       "      <td>1705948997</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>model</td>\n",
       "      <td>1677610602</td>\n",
       "      <td>openai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>model</td>\n",
       "      <td>1686587434</td>\n",
       "      <td>openai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>model</td>\n",
       "      <td>1677649963</td>\n",
       "      <td>openai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>model</td>\n",
       "      <td>1698959748</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt-3.5-turbo-16k</td>\n",
       "      <td>model</td>\n",
       "      <td>1683758102</td>\n",
       "      <td>openai-internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gpt-3.5-turbo-instruct-0914</td>\n",
       "      <td>model</td>\n",
       "      <td>1694122472</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tts-1</td>\n",
       "      <td>model</td>\n",
       "      <td>1681940951</td>\n",
       "      <td>openai-internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>davinci-002</td>\n",
       "      <td>model</td>\n",
       "      <td>1692634301</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gpt-3.5-turbo-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1692901427</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>babbage-002</td>\n",
       "      <td>model</td>\n",
       "      <td>1692634615</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tts-1-1106</td>\n",
       "      <td>model</td>\n",
       "      <td>1699053241</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>text-embedding-ada-002</td>\n",
       "      <td>model</td>\n",
       "      <td>1671217299</td>\n",
       "      <td>openai-internal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id object     created         owned_by\n",
       "0        gpt-3.5-turbo-16k-0613  model  1685474247           openai\n",
       "1                      dall-e-3  model  1698785189           system\n",
       "2        text-embedding-3-large  model  1705953180           system\n",
       "3                      dall-e-2  model  1698798177           system\n",
       "4                     whisper-1  model  1677532384  openai-internal\n",
       "5                 tts-1-hd-1106  model  1699053533           system\n",
       "6                      tts-1-hd  model  1699046015           system\n",
       "7            gpt-3.5-turbo-0125  model  1706048358           system\n",
       "8        text-embedding-3-small  model  1705948997           system\n",
       "9                 gpt-3.5-turbo  model  1677610602           openai\n",
       "10           gpt-3.5-turbo-0613  model  1686587434           openai\n",
       "11           gpt-3.5-turbo-0301  model  1677649963           openai\n",
       "12           gpt-3.5-turbo-1106  model  1698959748           system\n",
       "13            gpt-3.5-turbo-16k  model  1683758102  openai-internal\n",
       "14  gpt-3.5-turbo-instruct-0914  model  1694122472           system\n",
       "15                        tts-1  model  1681940951  openai-internal\n",
       "16                  davinci-002  model  1692634301           system\n",
       "17       gpt-3.5-turbo-instruct  model  1692901427           system\n",
       "18                  babbage-002  model  1692634615           system\n",
       "19                   tts-1-1106  model  1699053241           system\n",
       "20       text-embedding-ada-002  model  1671217299  openai-internal"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(openai.Model.list()['data'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-913axHvzeIS2OGFQ7KRVnYt4FvOQT at 0x24240869630> JSON: {\n",
       "  \"id\": \"cmpl-913axHvzeIS2OGFQ7KRVnYt4FvOQT\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1710039175,\n",
       "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\n\\nThe capital of India is New Delhi.\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 7,\n",
       "    \"completion_tokens\": 9,\n",
       "    \"total_tokens\": 16\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chat Completion API and Completion API\n",
    "\n",
    "## Completion API\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model ='gpt-3.5-turbo-instruct',\n",
    "    prompt = \"what is the capital of India?\"\n",
    "    \n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-914DAGbXunfgMbsdHlDdc0cLe9zxT at 0x242422f3950> JSON: {\n",
       "  \"id\": \"cmpl-914DAGbXunfgMbsdHlDdc0cLe9zxT\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1710041544,\n",
       "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\n\\nHey there! I am an AI assistant and I would be happy to explain machine learning to you in a way that is\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 16,\n",
       "    \"completion_tokens\": 25,\n",
       "    \"total_tokens\": 41\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_max = openai.Completion.create(\n",
    "    model ='gpt-3.5-turbo-instruct',\n",
    "    prompt = \"Act as an AI assistant and Explain machine learning to a 5 years old?\",\n",
    "    max_tokens=25\n",
    "    ## we can change the value of max_tokens based on that it will give conetent till that tokens\n",
    "    \n",
    ")\n",
    "\n",
    "response_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-914RPoJWE46kFOzTPTCSgCIyAbX8a at 0x24242315bd0> JSON: {\n",
       "  \"id\": \"cmpl-914RPoJWE46kFOzTPTCSgCIyAbX8a\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1710042427,\n",
       "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\n\\n1. London, United Kingdom\\n2. Paris, France\\n3. New York City, United States\\n4.\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    },\n",
       "    {\n",
       "      \"text\": \"\\n\\n1. London, United Kingdom\\n2. New York City, United States\\n3. Paris, France\\n4.\",\n",
       "      \"index\": 1,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    },\n",
       "    {\n",
       "      \"text\": \"\\n\\n1. London, United Kingdom\\n\\nLondon is a global hub for culture, finance, and tourism. It is home to\",\n",
       "      \"index\": 2,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    },\n",
       "    {\n",
       "      \"text\": \"\\n\\n1. Tokyo, Japan\\n2. London, United Kingdom\\n3. Paris, France\\n4. New York City\",\n",
       "      \"index\": 3,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    },\n",
       "    {\n",
       "      \"text\": \"\\n\\n1. Tokyo, Japan\\n2. London, United Kingdom\\n3. New York City, United States\\n4.\",\n",
       "      \"index\": 4,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"completion_tokens\": 125,\n",
       "    \"total_tokens\": 130\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_max = openai.Completion.create(\n",
    "    model ='gpt-3.5-turbo-instruct',\n",
    "    prompt = \"Top cities in the world\",\n",
    "    max_tokens=25,\n",
    "    temperature=0.6, # if it is 0 the model will not take any risk it will give same answer and best answer\n",
    "                    # if it is towards to 1 it will not give single output it will give multiple versions of outputs\n",
    "                    #always it will not give the biased results it will keep on changing withe different best answers\n",
    "    ## we can change the value of max_tokens based on that it will give content till that tokens\n",
    "    n=5 # it will give 5 outputs\n",
    "        # with out n value all outputs will give in one output\n",
    ")\n",
    "\n",
    "response_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-914Z6MOnaAoCambq0PUDc3OT9QjP2 at 0x242423150e0> JSON: {\n",
       "  \"id\": \"cmpl-914Z6MOnaAoCambq0PUDc3OT9QjP2\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1710042904,\n",
       "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\n\\nHear ye, hear ye! Allow me, your faithful AI assistant, to enlighten thee about the wondrous world\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    },\n",
       "    {\n",
       "      \"text\": \"\\n\\nHark, good sir/madam! I am an AI assistant, here to enlighten thee on the wondrous world\",\n",
       "      \"index\": 1,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    },\n",
       "    {\n",
       "      \"text\": \"\\n\\nHark! I am an AI assistant, here to shed light on the wondrous world of AI in the language of\",\n",
       "      \"index\": 2,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    },\n",
       "    {\n",
       "      \"text\": \"\\n\\nGreetings, dear user! I am your AI assistant, here to serve and enlighten you. Today, I shall enlight\",\n",
       "      \"index\": 3,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 15,\n",
       "    \"completion_tokens\": 100,\n",
       "    \"total_tokens\": 115\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_ai = openai.Completion.create(\n",
    "    model ='gpt-3.5-turbo-instruct',\n",
    "    prompt = \"Act as an AI assiatnt and explain about AI in shakespeare word\",\n",
    "    max_tokens=25,\n",
    "    temperature=0.6, # if it is 0 the model will not take any risk it will give same answer and best answer\n",
    "                    # if it is towards to 1 it will not give single output it will give multiple versions of outputs\n",
    "                    #always it will not give the biased results it will keep on changing withe different best answers\n",
    "    ## we can change the value of max_tokens based on that it will give content till that tokens\n",
    "    n=4 # it will give 5 outputs\n",
    "        # with out n value all outputs will give in one output\n",
    ")\n",
    "\n",
    "response_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hear ye, hear ye! Allow me, your faithful AI assistant, to enlighten thee about the wondrous world\n"
     ]
    }
   ],
   "source": [
    "print(response_ai['choices'][0]['text'])\n",
    "# choices :- from the output response list of senetence outputs stored in this choices\n",
    "# [0] :- 1st sentence from the list of outputs in choices\n",
    "# text :-  specific 1st output statement from the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-914oWKpnJwcttjiFs1DgLzfp7Ko4W\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1710043860,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\n\\nNegative\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 24,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 27\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response_tweets = openai.Completion.create(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"\"\"Act as an AI model to detect positive or negative tweets:\n",
    "    This person is bad and he is biased towards negative sentiments\"\"\",\n",
    "    max_tokens=15,\n",
    "    temperature=1,\n",
    "    n=1   \n",
    "    \n",
    ") #this is like prompt engineer , it will predict either positive or negative based i=on given sentence\n",
    "print(response_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-914uUD2Simo3AqJMY6ZFpwZsruolO\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1710044230,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\nBonjour comment allez-vous ?\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 17,\n",
      "    \"completion_tokens\": 7,\n",
      "    \"total_tokens\": 24\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response_language_translation = openai.Completion.create(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"\"\"Act as an AI model to translate text to French:\n",
    "             Hello How are you ?\"\"\",\n",
    "    max_tokens=15,\n",
    "    temperature=1,\n",
    "    n=1   \n",
    "    \n",
    ") #this is like prompt engineer ,it will convert the given input to given language\n",
    "print(response_language_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9155EX1xoNT6fFYgvEli5uUclXyNo\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1710044896,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Hello! I'm just a computer program, so I don't have feelings, but I'm here to help you. How can I assist you today?\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 21,\n",
      "    \"completion_tokens\": 31,\n",
      "    \"total_tokens\": 52\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_2b778c6b35\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#chat Completion API -- it is a conversational API, it is sutalble for chatbot\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages=[{'role':'system','content':\"Act as an AI Assistant\"},\n",
    "              {'role':'user','content':\"Hello How are You?\"}]\n",
    ")\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between completion and chat completion\n",
    "\n",
    "#completion --> here we can able to give one prompt at a time\n",
    "#chat completion --> here we can able to give n number of messages ad we caable to append it,\n",
    "#                     the entire context will remeber it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-915SAfx2KUH5aLb7ZcVDVtQzUlpWM\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1710046318,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Based on the information you provided, your age is 33 years old.\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 44,\n",
      "    \"completion_tokens\": 15,\n",
      "    \"total_tokens\": 59\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_4f0b692a78\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#chat Completion API -- it is a conversational API, it is sutalble for chatbot\n",
    "chat_response_1= openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages=[{'role':'system','content':\"Act as an AI Assistant\"},\n",
    "              {'role':'user','content':\"Hello How are You?\"},\n",
    "              {'role':'user','content':\"My age is 33 and I am a Data Scientist\"},\n",
    "              {'role':'user','content':\"what is my age\"}\n",
    "              ]\n",
    ") # n number of messages and question we can able to given\n",
    "print(chat_response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-915VcOKSx0FZiqye7Bykdnfv7fR3k\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1710046532,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Your designation is Data Scientist.\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 54,\n",
      "    \"completion_tokens\": 6,\n",
      "    \"total_tokens\": 60\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_4f0b692a78\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#chat Completion API -- it is a conversational API, it is sutalble for chatbot\n",
    "chat_response_1= openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages=[{'role':'system','content':\"Act as an AI Assistant\"},\n",
    "              {'role':'user','content':\"Hello How are You?\"},\n",
    "              {'role':'user','content':\"My age is 33 and I am a Data Scientist\"},\n",
    "              {'role':'user','content':\"what is my age\"},\n",
    "              {'role':'user','content':\"what is my disignation\"}\n",
    "              ]\n",
    ") # n number of messages and question we can able to given\n",
    "print(chat_response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-915VqmXTHgqIhpGkiYZf6fD4It6nt\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1710046546,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Your role is Data Scientist, and your age is 33.\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 52,\n",
      "    \"completion_tokens\": 13,\n",
      "    \"total_tokens\": 65\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_4f0b692a78\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#chat Completion API -- it is a conversational API, it is sutalble for chatbot\n",
    "chat_response_1= openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages=[{'role':'system','content':\"Act as an AI Assistant\"},\n",
    "              {'role':'user','content':\"Hello How are You?\"},\n",
    "              {'role':'user','content':\"My age is 33 and I am a Data Scientist\"},\n",
    "              {'role':'user','content':\"what is my age\"},\n",
    "              {'role':'user','content':\"what is my role\"}\n",
    "              ]\n",
    ") # n number of messages and question we can able to given\n",
    "print(chat_response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-92787fJ8mRg71GEwMK3v1RXKdTQ7Y\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1710291091,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\nThere is nothing technically wrong with this code, but it is not producing the desired output of \\\"Hello Pratyuhsa.\\\" The variable name is misspelled, so it is printing \\\"Hello Pratyuhsa\\\" instead.\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 27,\n",
      "    \"completion_tokens\": 49,\n",
      "    \"total_tokens\": 76\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt_msg = \"\"\"\"Act as an AI code to explain what is wrog in the code:\n",
    "Code:print(\"Hello Pratyuhsa\")\n",
    "\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt = prompt_msg,\n",
    "    max_tokens=100,\n",
    "    temperature=1,\n",
    "    n=1\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§π‡•à‡§Ç, ‡§Æ‡•à‡§Ç ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•Å‡§∑‡§æ ‡§π‡•Ç‡§Å, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•ã‡•§ ‡§Ü‡§™ ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç?\n"
     ]
    }
   ],
   "source": [
    "prompt_msg = \"\"\"\"Act as an AI code to translate the given text to hindhi:\n",
    "Hi Hello, I am pratyusha, how are you? What are you doing?\n",
    "\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt = prompt_msg,\n",
    "    max_tokens=100,\n",
    "    temperature=1,\n",
    "    n=1\n",
    ")\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∞π‡∞æ‡∞Ø‡±ç, ‡∞®‡±á‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±Ç‡∞∑, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å? ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞è‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?\n"
     ]
    }
   ],
   "source": [
    "prompt_msg = \"\"\"\"Act as an AI code to translate the given text to Telugu:\n",
    "Hi Hello, I am pratyusha, how are you? What are you doing?\n",
    "\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt = prompt_msg,\n",
    "    max_tokens=200,\n",
    "    temperature=1,\n",
    "    n=1\n",
    ")\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy: üòä\n",
      "Sad: üòî\n",
      "Smily: üòÉ\n",
      "Cry: üò¢\n"
     ]
    }
   ],
   "source": [
    "prompt_msg = \"\"\"\"Act as an AI code to convert message to emoji:\n",
    "Happy\n",
    "Sad\n",
    "Smily\n",
    "Cry\n",
    "\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt = prompt_msg,\n",
    "    max_tokens=200,\n",
    "    temperature=1,\n",
    "    n=1\n",
    ")\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëãüèæ ‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç, ‡∞®‡±á‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±Å‡∞∑, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å? ‡∞Æ‡±Ä ‡∞ï‡±É‡∞§‡∞ú‡±ç‡∞û‡∞§‡∞≤‡±Å ‡∞è‡∞Ç‡∞ü‡±ã ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞Ç‡∞°‡∞ø? \n",
      "‡∞®‡±á‡∞®‡±Å ‡∞∏‡∞Ç‡∞§‡±ã‡∞∑‡∞Ç‡∞ó‡∞æ ‡∞â‡∞Ç‡∞¶‡∞ø üòä\n"
     ]
    }
   ],
   "source": [
    "prompt_msg = \"\"\"\"Act as an AI code to translate to Telugu and convert message to emoji:\n",
    "Hi Hello, I am pratyusha, how are you? What are you doing? \n",
    "I am Happy\n",
    "\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt = prompt_msg,\n",
    "    max_tokens=200,\n",
    "    temperature=1,\n",
    "    n=1\n",
    ")\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing in Amazon Web Services (AWS) refers to the process of using the cloud computing platform to access and manage various computing resources, such as servers, storage, databases, and software applications. This allows users to run their applications using Amazon's infrastructure, without the need for physical hardware. AWS provides a pay-as-you-go model, scalability, and a wide range of services, making it a popular choice for businesses and individuals looking to utilize cloud computing for their computing needs.\n"
     ]
    }
   ],
   "source": [
    "prompt_msg = \"\"\"\"Act as an AI code to summarize the content:\n",
    "what is compute in aws?\n",
    "\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt = prompt_msg,\n",
    "    max_tokens=1000,\n",
    "    temperature=1,\n",
    "    n=1\n",
    ")\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
